\chapter{Introduction}\label{ch:intro}
%!TEX root = main.tex

\begin{quote}
{\em Life's most important questions are, for the most part, nothing but probability problems.} - Laplace
\end{quote}

\section{Why write a book like this?}

This book is written to address the role that the mathematics of probability can play when applied to topics in religion.  Specifically, we have found that there are two primary purposes of this approach:
\be
\i Bring clarity to other similar treatments of these topics.  The mathematics of probability have, unfortunately, been used to give the veneer of authority and objectivity to Christian apologetic arguments that are not well supported\cite{swinburne2003resurrection}.  This is typically done by sneaking, possibly inadvertently, a bad assumption into an otherwise correct analysis.  Understanding the structure of the mathematics can help in correcting this.
\i Bring concreteness and lucidity to more verbose and {\em murky} approaches.  When talking about terms like {\em faith}, {\em miracles}, and {\em evidence}, the mathematics forces the analysis to be both specific and complete, while at the same time having the benefit of reducing the number of symbols used in the description.  Thus, there is an economy of words that is achieved.  Pages of philosophical exposition can often be summarized by a few equations\cite{howard2013propositional}.  This makes it much easier to explore special cases, and to see where analogies are successful and where they fail.  This process gives the reader the ability to see connections between concepts, even when they seem opposed.
\ee

\section{Who is this for?}

Although we intend this book to be technical, we do not want to scare away those who are less mathematical.  In fact, we would consider it a success if someone who is not particularly inclined in the mathematical arts would be able to get a new appreciation for these topics upon reading this book.  Therefore, we will try to limit the technical aspects to those that are only absolutely necessary, and will rely heavily on specific examples at all times.

\section{Organization}

In this chapter we outline the basics of probability, applied to a few simple cases.  We then explore in subsequent chapters the details of specific concepts, including {\em faith}, {\em miracles}, and {\em historical methods}.  Typically, these explorations center around responses to particular works, podcast discussions and debates.  


\section{Introduction to Probability}

When we speak about probability, we speak about a percentage chance (0\%-100\%) for something to happen, although we often write the percentage as a decimal number, between 0 and 1.  If the  probability of an event is 0 then it is the same as saying that {\em you are certain that the event will never happen}.  If the probability is 1 then {\em you are certain that it {\em will} happen}.  Life is full of uncertainty, so we assign a number somewhere between 0 and 1 to describe our state of knowledge of the certainty of an event. The probability that you will get struck by lightning sometime in your life is $p=0.0002$, or 1 out of 5000. Statistical inference is simply the inference in the presence of uncertainty.  We try to make the best decisions we can, given incomplete information.  

Pierre-Simon Laplace, who first formalized the mathematics of probability, spoke of an agent with perfect knowledge.  This agent, Laplace claimed, would not need probability at all.
\pquote{We may regard the present state of the universe as the effect of its
past and the cause of its future. An intellect which at a certain moment
would know all forces that set nature in motion, and all positions of
all items of which nature is composed, if this intellect were also vast
enough to submit these data to analysis, it would embrace in a single
formula the movements of the greatest bodies of the universe and those
of the tiniest atom; for such an intellect nothing would be uncertain
and the future just like the past would be present before its eyes.\cite{laplace1825philosophical}}

E.T. Jaynes describes it in much the same way.  He says that we label something ``random'' due to our ignorance of the system not to any intrinsic randomness.  He called this labeling the mind-projection fallacy\cite{Jaynes2003}, where you misattribute the unpredictable behavior of a system as a product of the system itself. A rolled die is following the laws of physics, deterministically, and detailed knowledge of the die, the roll, and the surface should allow you to predict 100\% of the time what it will do.  We lack that knowledge, thus the behavior becomes unpredictable. We often then attribute that unpredictable behavior as a ``random die'', as if it were the die that contains the randomness and not our own ignorance.


\subsection{The Basic Rules of Probability}\label{sec:rules}

For a complete description of these rules, and their application in general statistical inference there are several books available, one of which from the present author\cite{Blais:2014aa}.  We will need to establish a basic set of notation and mathematics in order to address the concepts.  This notation will in some cases make clear and condensed (due the the terseness of mathematics) much longer expositions of the same concepts in English.  In other cases it will provide a systematic framework for exploring disparate problems, in order to see the connection to all of rational thought.  We begin by describing the rules of probability, and some of their consequences.

\subsubsection{Rule 1 (Definition rule):}

$P(A)$ is a number between 0 and 1, representing the strength of belief in a statement, $A$.

An example is 
\beqn
P(\mbox{you will get struck by lightning sometime in your life}) = 0.0002
\eeqn
Although this probability is \emph{estimated} by the fraction of people being struck by lightning, whenever we write probabilities we are referring to a measure of the strength of one's belief in a proposition.  Thus, this probability means that you believe it to be extremely unlikely that you will be struck by lightning in your life.  This belief, as we will see, is not just a {\em guess} but is something arrived at through proper rational processes, or in other words, by adhering strictly to the rules of probability.

In terms of economy of symbols\marginnote{Because this is just a shorthand, we find it useful when the formalism gets too abstract to insert whatever statement for the shortcut you want.  Thus, it helps your intuition to put in a specific example with each equation.}, we will often define a single symbol to represent an entire sentence, or collection of sentences.  For example,
\beqn
H&\equiv&\left\{\parbox{2in}{``you will get struck by lightning sometime in your life''}\right. \\
P(H)&=&0.0002
\eeqn
means that you believe it to be extremely unlikely that you will be struck by lightning in your life.

\subsubsection{Rule 2 (Negation rule):}
\beqn
P(A) + P({\rm\bf not}\, A) = 1
\eeqn
In other words, either a statement is true or its negation is true.
\beqn
H&\equiv&\left\{\parbox{1.5in}{``you will get struck by lightning sometime in your life''}\right. \\
P(H)+P({\rm\bf not}\, H)&=&1
\eeqn
means that you can be {\em certain} that either you will be struck by lightning in your life or you won't.  This seems rather obvious, and you may be wondering why we even bring it up, but it becomes a source of one of the most common logical fallacies - the either-or fallacy. 

Notice how this occurs.  The following is correct logical inference:
\beqn
B&\equiv&\left\{\parbox{1.5in}{``a playing card drawn from a deck is black''}\right. \\
P(B)+P({\rm\bf not}\, B)&=&1
\eeqn
means that, if you draw a playing card from a deck, you can be {\em certain} that it is either black or it is not-black.  This is true no matter what kind of deck of cards you are dealing with.  The following, however, is not correct logical inference:
\beqn
B&\equiv&\left\{\parbox{1.5in}{``a playing card drawn from a deck is black''}\right. \\
R&\equiv&\left\{\parbox{1.5in}{``a playing card drawn from a deck is red''}\right. \\
P(B)+P(R)&=&1 \leftarrow\mbox{\underline{\em this is incorrect}}
\eeqn
The key point here is that ``not-black'' is not the same as ``red'', except in those cases where you can be certain that there are only those two possibilities.  One has to be on the lookout for hidden possibilities - perhaps one has a {\em Five Crowns} deck which has green and yellow cards as well.  Failure of imagination can easily lead to accidental either-or logical failures\footnote{An example seen later is an argument by a Christian examining the probability of theism ($T$) being true or atheism ($A$) being true.  The argument hinges on $P(T)+P(A)=1$, and trying to reduce $P(A)$ to win the argument.  However, the proposition ``theism is true'' masks as many alternatives as ``not-black'' in the example here.  One could have the Christian God, the Greek pantheon, the Cthulhu mythological structure, etc... and thus the argument does not perform what the Christian thinks it does.}.

\subsubsection{Rule 3 (Conjunction rule):} \beqn
\P{$A$ and $B$} = P(B|A)P(A)
\eeqn
which is the probability of two statements both being true, A {\bf and} B.  We define a new symbol, $|$, which should be read as ``given.''  When there is information given, we call this probability {\em conditional} on that information. 
\beqn
H&\equiv&\left\{\parbox{1.5in}{``you will get struck by lightning sometime in your life''}\right. \\
G&\equiv&\left\{\parbox{1.5in}{``you like to play golf in the rain''}\right. \\
\P{$H$ and $G$} &=& P(H|G)P(G)
\eeqn
means that the chance of you getting struck by lightning in your lifetime {\bf and} you like to play golf in the rain is related to the probability of you liking to play golf in the rain ($P(G)$) and the probability that you will get struck by lightning in your lifetime {\em given that} you like to play golf in the rain ($P(H|G)$).  There are a few points to be made here, which become important in later examples.
\be
\i Notice how concise the description is - the math can summarize the relationship between several concepts with few words or symbols.  
\i $P(H|G)$ is probably higher than $P(H)$.  In other words, the fact that a person likes to play golf in the rain makes it more likely that they will be struck by lightning in their lifetime.
\i Even if we don't have specific numbers, we can still reason about which probabilities are larger or smaller, or which ones are important or not.
\i $\P{$H$ and $G$}$ is\marginnote{Sam Harris likes to humorously point out that Mormonism is {\em objectively} less likely than Christianity because Mormonism is Christianity {\bf and} a number of implausible statements.   This, however, is somewhat misleading because Christianity is also predicated on those so-called implausible statements being false.  One has to be careful when constructing the probabilities!} {\em always} less than $P(G)$ - the conjunction of two things is inherently (and mathematically) less probable than the individual components.
\ee 
\subsubsection{Rule 4 (Bayes' rule):}\label{sec:rule4} This rule is perhaps the most obtuse to see for the first time, but is by far the most important rule of them all, so it is worth the effort.  Because of this, we will choose to write it in a somewhat more elaborated form, and rewrite it several ways.
\beqn
\Pg{explanation}{data} &=& \frac{\Pg{data}{explanation}\P{explanation}}{\P{data}}
\eeqn
where each term is described more fully as 
\bi
\i $\P{explanation}$ - this is the probability the explanation is correct {\em prior} to seeing the data.  The term itself is often called the {\em prior}, and represents your beliefs before you see the data.  Typically, more complex explanations are less likely a-{\em prior}i than simpler ones.  
\i $\Pg{explanation}{data}$ - this is the probability the explanation is correct {\em after} seeing the data (a-{\em posteriori}). The term itself is often called the {\em posterior} for this reason, and represents your updated beliefs once you have data.  Thus, Bayes' rule is a mathematical expression of {\em learning} from evidence.  
\i $\Pg{data}{explanation}$ - this is the probability that the data can be explained with this particular explanation.  The term itself is often called the {\em likelihood}, and can be thought of as a measure of how well the explanation fits the data.  If the explanation fits the data well, this number will be high, for example. 
\i $\P{data}$ - this is the probability of the data, regardless of the explanation.  It is easiest to understand this term with an example.
\ei

Imagine we are playing a game with several small decks of cards\marginnote{The analogy here is that the universe is set up with a set of rules that we are trying to determine.  Thus, deciding on which deck we are holding is analogous to deciding which universe we are in given my observations of the universe.  In other words, providing an {\em explanation} of the data is really about determining which of the many possible universes we are in.}, defined here:
\be
\i $E_1: A\spades, 2\hearts, 3\spades,4\spades$
\i $E_2: A\hearts, 2\hearts, 3\spades, 4\spades$
\i $E_3: A\hearts,2\hearts, 2\hearts,4\spades$
\i $E_4: A\hearts,3\spades, 3\spades, 4\spades$
\ee
In the game, someone has handed us one of the decks, but we don't know at all which one it is.  We then draw the top card, observe that it is a $3\spades$, and see if we can reason about which deck is likely to be the one we are holding.  We choose such a small, simple system because it is easy to intuit the answers without the math.  This intuition can provide a scaffold for understanding the mathematics, which can be used in more complex examples where one {\em doesn't} have a strong intuition.  It is therefore worth going through at least one example in detail.

To begin, we need to assign the probabilities of the four cases {\em prior} to the data.  Given total ignorance, we assign equal probabilities to the four cases
\beqn
P(E_1)&=&1/4 \\
P(E_2)&=&1/4 \\
P(E_3)&=&1/4 \\
P(E_4)&=&1/4
\eeqn
The following are our intuitions, with the mathematics in parallel below. Since we drew a $3\spades$, our intuition says that this should rule out $E_3$ altogether.  Further, it says $E_4$ should be more likely than the other remaining two because it contains the observed card, $3\spades$, more than one time - it is easier to get that particular card from the fourth deck than the others.

The mathematics would look like this
\beqn
P(E_1|3\spades)&=&\frac{P(3\spades|E_1)P(E_1)}{P(3\spades)}\begin{array}{c}\ \\\leftarrow\mbox{this term the same in all}\end{array}\\
P(E_2|3\spades)&=&\frac{P(3\spades|E_2)P(E_2)}{P(3\spades)}\\
P(E_3|3\spades)&=&\frac{P(3\spades|E_3)P(E_3)}{P(3\spades)}\\
P(E_3|3\spades)&=&\frac{P(3\spades|E_4)P(E_4)}{P(3\spades)}
\eeqn
where we already have
\beqn
P(E_1)=P(E_2)=P(E_3)=P(E_4)=1/3
\eeqn
Further, we have 
\beqn
P(3\spades|E_1) = 1/4
\eeqn
because one card out of 4 in the first deck is the $3\spades$.  Likewise, we have
\beqn
P(3\spades|E_2) &=& 1/4\\
P(3\spades|E_3) &=& 0 \\
P(3\spades|E_4) &=& 2/4 
\eeqn
Finally we have
\beqn
P(3\spades) = 4/16
\eeqn
because there are ``$3\spades$'' cards out of all the 16 in the game.  Plugging these numbers in to the above equations, and performing the arithmetic, we have
\beqn
P(E_1|3\spades)&=&\frac{(1/4)\times (1/4)}{(4/16)} = 1/4 \\
P(E_2|3\spades)&=&\frac{(1/4)\times (1/4)}{(4/16)} = 1/4 \\
P(E_3|3\spades)&=&\frac{(0)\times (1/4)}{(2/12)} = 0 \\
P(E_4|3\spades)&=&\frac{(2/4)\times (1/4)}{(4/16)} = 1/2 \\
\eeqn
which perfectly matches our intuition.  Notice further that $P(3\spades|E_1)$ is another way of saying ``how well is the observation of a $3\spades$ explained by the idea that we're holding the first deck?''  The entire process can then be thought of as updating our initial beliefs with the new evidence.  
\begin{quote}
{\em Any process of reasoning, in any field whatsoever, is either consistent with this process of calculation or it is not rational.}
\end{quote}
It is for this reason that we explore this process in such detail.

On another front, an alternate way to have calculated the shared bottom term, $P(3\spades)$, is the following
\beqn
P(3\spades) &=& P(3\spades|E_1)P(E_1) + P(3\spades|E_2)P(E_2)+ \\
&&P(3\spades|E_3)P(E_3)+ P(3\spades|E_4)P(E_4)\\
&=&(1/4)\times (1/4) + (1/4)\times (1/4) + (0)\times (1/3)+ (2/4)\times (1/4) \\
&=& 4/16
\eeqn
Why would one write it in this seemingly long-winded and complex way?  Because it makes it easier to say, in words, what this term is doing.  It is the sum of all of the probabilities for how well each explanation accounts for the data scaled by how likely that explanation was before seeing the data.  In other words, proper rational inference requires that you re-weight the strength of your beliefs in an explanation not just by how well that explanation describes your observations, but also by how intrinsically likely that explanation is before your observations and how well all of the {\em alternatives} perform on those same observations.  An observation can be very well explained by a particular explanation, but if it can be equivalently explained by other, simpler, explanations, then your belief in that more complex  explanation may in fact {\em weaken} with the new observation (i.e. it's probability could go down).

\section{On Simplicity}

Ockham's razor, which is the philosophical idea that simpler theories are preferred, is a consequence of Bayes' rule when comparing models of differing complexity\cite{jefferys1991sharpening}.  We can see this by extending the card game example with a fifth possibility.  Instead of giving the specific cards in this deck, we are simply told
\be
\i $E_5:$ the deck can have anywhere from zero to three $3\spades$, and enough other cards to make a total of four cards
\ee
This explanation of the game is plastic\marginnote{In mathematical models, this is often referred to as having an {\em adjustable parameter} - a value in the model that is not specified ahead of time, but can be {\em fit} to the data, and an optimum value found.}
 - depending on the data, we may infer a different value for the number of $3\spades$ in this deck.  It may be heavily loaded toward $3\spades$, which would make $E_{5}$ explain the data very well; however it may have none, and not explain the data at all.  Clearly, once you observe a $3\spades$, the ``best'' value for this deck is to have three of them out of the four cards - making it more likely than the previously best explanation, $E_4$, which only had two our of four.
 
However, this process of reasoning violates the laws of probability by not taking our uncertainty of this parameter (i.e. the number of $3\spades$  in the deck) into account.  For simplicity, let's just consider the two decks in question, $E_4$ and $E_5$, and play the game with them (again, as before, drawing a $3\spades$ from the top).
\be
\i $E_4: A\hearts,3\spades, 3\spades, 4\spades$
\i $E_5:$ the deck can have anywhere from zero to three $3\spades$, and enough other cards to make a total of four cards
\ee
We set up the calculation as before
\beqn
P(E_4|3\spades)&=&\frac{P(3\spades|E_4)P(E_4)}{P(3\spades)}\\
P(E_5|3\spades)&=&\frac{P(3\spades|E_5)P(E_5)}{P(3\spades)}
\eeqn
We defer the calculation of the shared term, $P(3\spades)$, and focus on the numerators of both calculations.\marginnote{Once we have the numerators, we can add them up to get the shared term $P(3\spades)$}  First the one for $E_4$ (and remember, we have only two decks here),
\beqn
P(E_4|3\spades)&\sim&P(3\spades|E_4)P(E_4)\\
&=&(2/4)\times (1/2) = 1/4
\eeqn
Next with the $E_5$ deck,
\beqn
P(E_5|3\spades)&\sim&P(3\spades|E_5)P(E_5)\\
&=&\underbrace{P(3\spades|E_5)}_{\parbox{1.4in}{broken up into the four possibilities from zero to three $3\spades$}}\times (1/2) \\
\eeqn
Each of the possibilities (all equally likely, because we are given no other information) has the form of the fraction of $\spades$ for that possibility times 1/4 because there are 4 total possibilities to consider, for example
\beqn
P(3\spades|E_5\mbox{ {\bf and zero }} 3\spades)P(\mbox{zero } 3\spades|E_5) &=& \underbrace{(0/4)}_{\mbox{\#}\spades} \times (1/4)
\eeqn
Doing the same for all the possibilities, we get for the $E_5$ numerator,
\beqn
P(3\spades|E_5)P(E_5) &=&\left[(0/4)\times (1/4) +  (1/4)\times (1/4) +  (2/4)\times (1/4) +  (3/4)\times (1/4)\right]\times (1/2) \\
&=&3/16
\eeqn
Finally, we can get the shared term, 
\beqn
P(3\spades)= 1/4 + 3/16 = 7/16
\eeqn
and the probabilities of each of the decks, given the observation of a $P(3\spades)$,
\beqn
P(E_4|3\spades)&=&\frac{1/4}{7/16} = 4/7\\
P(E_5|3\spades)&=&\frac{3/16}{7/16} = 3/7
\eeqn
This means that, although $E_5$ contains the {\em possibility} of a better fit to the data, it is less {\em probable} because it has a flexible parameter that is unspecified {\em before} the data.

When we prefer a ``simpler'' model with Ockham's razor, simpler means fewer such adjustable parameters.  It also means that the predictions are both {\em specific} and not {\em overly plastic}. For example, a hypothesis which is consistent with the observed data, and also be consistent if the data were the opposite would be overly plastic.  An example of an infinitely plastic ``explanation'' is {\em magic did it}.  Because it can ``explain'' anything, given that it is consistent with any possible observation, it explains nothing.


\section{On Belief, Knowledge, and Proof}

Clearly the terms belief, knowledge, and proof are related but it is the framework of probability that helps us be specific about their differences.  We can summarize the situation with the following,
\bi
\i \emph{Belief} is measured by probability.  Higher probability is equivalent to stronger belief, and likewise, lower probability is equivalent to weaker belief.
\i \emph{Knowledge} is a subset of belief, described more specifically below.
\i \emph{Proof} refers specifically to the result of a \emph{deductive} process.  In this process, one starts with some (given) axioms, and can demonstrate \emph{with certainty} (i.e. prove) a number of theorems (i.e direct consequences) from those axioms.  Thus, the word proof should only be used in situations involving \emph{certainty}, or in other words, probabilities of exactly 1 or exactly 0 only.
\ei

\subsection{Belief}

\begin{quote}
We say we believe a proposition $A$ when $P(A)>0.5$.  We say we
believe \emph{strongly} in a proposition $A$ when $P(A)>0.95$
or some other, somewhat arbitrary, high number.  The strength of a
belief is a \emph{scale} measured my the probability assigned to that proposition.
\end{quote}

Everything we have covered so far in this book relates to belief.

\subsection{Knowledge}

What is knowledge?  Plato gave the following definition of knowledge:

\begin{quote}
Knowledge is justified true belief.\cite{fine2003plato}
\end{quote}

This is an unsatisfying definition because, it seems, in
order to justifiably label anything as \emph{knowledge} with this
definition we'd need to be able to independently determine that the
proposition is true. This presupposes that there is some ``outside''
knowledge, which we feel comes too close to assuming a religious
justification at the outset.  We believe there is likely to be a truth to be known, but that we can never truly know what it is for certain - but this is not a problem. It is a red herring to bring up 100\% certainty for knowledge, because it is never achievable, and isn't what we practically call knowledge. We prefer a definition inspired by Stephen J Gould:\marginnote{In this way, knowledge is a subset of belief.}

\begin{quote}
In science, `fact' can only mean `confirmed to such a degree that it
would be perverse to withhold provisional assent. `I suppose that apples
might start to rise tomorrow, but the possibility does not merit equal
time in physics classrooms.'\cite{gould1981evolution}
\end{quote}

Where it says ``fact'', read ``knowledge''.  Where it says ``science''
read ``life''.  The fact, or knowledge, that the Sun rises in the east and sets in 
the west is not due to a formal \emph{proof} that this is always true, or will 
continue indefinitely into the future.  It is an admission that the probability is so 
outrageously high, given the evidence of every other sun rise and sun set observed, 
and the further confirmation of models of the solar system, that it would be ``perverse to withhold provisional assent''.  We accept the claim as a practical matter, despite not being 100\% certain, because of the overwhelming probabilities.  This we call knowledge.  It is thus never a problem to admit lack of certainty in knowledge, and it can be seen as an obvious diversion if anyone tries to argue in a manner that suggests it's a problem.

Mathematically, we might write it as $P(A)>0.9999 \approx 1$\marginnote{$P(A)\approx 1$ means that the probability of $A$ is {\em approximately} equal to 1}.  Notice that we don't need 100\% certainty to claim knowledge, and that it is possible for the ``knowledge'' to be wrong (although, by definition, it is highly unlikely for this to be the case).  

We can ask the question, how did we come to this knowledge?  The answer is simply, by applying the rules of probability!  According to Bayes' rule, we update our probabilities given the evidence.  This can lead us to approach, but never equal, probability of 1.  We can approach, but never achieve, complete certainty of any claim.  Rationality only insists that we apply the rules of probability systematically. 

\subsection{Proof}

Do {\em proof} and {\em evidence} mean the same thing?  No, they don't. To summarize,
proof refers specifically to the result of a \emph{deductive} process.  In this process, one starts with some (given) axioms, and can demonstrate \emph{with certainty} (i.e. prove) a number of theorems (i.e direct consequences) from those axioms.  Thus, the word proof should only be used in situations involving \emph{certainty}, or in other words, probabilities of exactly 1 or exactly 0 only.

Evidence, however, factors into inference through an \emph{inductive} process, not \emph{deductive}.  Induction is just another name for the application of the rules of probability, so we have been doing that from the beginning of the book.  Deduction is just another name for formal logic, where the probabilities are restricted to values of 0 or 1 only.  As a result, we have the following maxim,


\begin{quote}
\emph{Proof} does not exist in science, only in math and philosophy.
\end{quote}

The only place you can have proof is where you have \emph{axioms}
(i.e. unprovable assertions), and can then \emph{prove} a number of
consequences of those axioms. We can prove, for example, that the sum of
the angles of a triangle is 180 degrees, if we start with the Euclidean
axioms of geometry. Science doesn't have axioms, and thus there are no
proofs - there is only evidence. We sometimes hear the term ``proven
scientifically'', even from people who should know better\footnote{An example of someone who should know better is Richard
Carrier in his \href{http://www.youtube.com/watch?v=YLvWz9GQ3PQ}{``Is
Philosophy Stupid'' talk}, his
\href{http://www.amazon.com/Proving-History-Bayess-Theorem-Historical/dp/1616145595}{books},
and his
\href{http://infidels.org/library/modern/richard_carrier/theory.html}{articles})}.

Another way of putting it is that probability theory contains deductive logic as a special case.  As it applies to science, it has the following consequence

\begin{quote}
All of the evidence in the universe cannot bring the probability of a
scientific claim to certainty.
\end{quote}

To see this directly, imagine we have two (and only two) hypotheses for
the Earth - flat earth and round earth. We can write Bayes' rule for the
probability of each given the data. Note that this data includes things
like the experience of airplane flight, Magellan's trip, pictures from
space - all of which could be faked! The flat earth hypothesis is not
\emph{logically} impossible, it's just been overwhelmed by the evidence.

\begin{eqnarray*}
P({\rm round}|{\rm data}) &=& \frac{P({\rm data}|{\rm round})P({\rm round})}{P({\rm data}|{\rm flat})P({\rm flat})+P({\rm data}|{\rm round})P({\rm round})}\\\\
P({\rm flat}|{\rm data}) &=& \frac{P({\rm data}|{\rm flat})P({\rm flat})}{P({\rm data}|{\rm flat})P({\rm flat})+P({\rm data}|{\rm round})P({\rm round})} 
\end{eqnarray*}

Notice that, no matter how well the round earth hypothesis explains the
data, \[P({\rm data}|{\rm round})\approx 1\] and how unlikely you
believe it is that the Earth is flat even \emph{before} the data,
\[P({\rm flat})\ll 1\] as long as there is some \emph{possible} (even if
seriously contrived) way to explain the data with the flat earth
hypothesis, \[P({\rm data}|{\rm flat}) \neq 0\] it is
\emph{mathematically impossible} to make the round earth hypothesis
certain, \[P({\rm round}|{\rm data})< 1\]

On the right-hand side of Bayes' Rule, the closer the round-earth terms
and the flat-earth terms get to 1 and 0, respectively, the closer the
left-hand terms get to 1 and 0, but they never \emph{equal} 1 and 0.
This does not mean that we can't be \emph{confident} of claims, only
that we cannot have \emph{absolute certainty of anything} in science
(and therefore, in life in general). Anyone who doesn't understand that
does not understand science.

For it to be {\em reasonable to believe in something}, it must rise to a level of probability that you would label it as {\em belief}.  Does this ever happen, or should this ever happen, with untrue things?  Certainly.  Here are a few that come to mind.

\be
\i the world is flat - as long as you are constrained to not live near the shore, or see a lunar eclipse
\i life is designed - before the advent of Darwin's theory of natural selection
\i the Sun, and the stars, all go around the Earth - until the advent of physics
\ee

In each of these cases there is in fact strong evidence for the claims, and against the counter claims, to make it reasonable to believe them (at the time).  It is no longer reasonable to believe these claims - the process of reason forces one to adjust the probabilities of the hypotheses given new evidence, and to discard those hypotheses that become too improbable.


\section{Lessons from Probability}
The mathematics of probability theory is the gold standard for all statistical inference.  It structures all inference in a systematic fashion.  However, it can be used without doing any calculations, as a guide to qualitative inference.  Some of the lessons that are consequences of probability theory are listed here, and will be noted throughout this text in various examples.

\bi
\i Confidence in a claim should scale with the evidence for that claim - the more evidence the higher confidence.
\i Ockham's razor, which is the philosophical idea that simpler theories are preferred, is a consequence of Bayes' Rule when comparing models of differing complexity that explain the data equally.
\i A complex model must explain the data \emph{better} than a simple model to be preferred.
\i Simpler means fewer adjustable parameters.
\i Simpler also means that the predictions are both {\em specific} and not {\em overly plastic}. For example, a hypothesis that is consistent with the observed data, and can also be consistent if the data were the opposite, would be overly plastic.
\i Your inference is only as good as the hypotheses (i.e. models) that you consider.
\i Extraordinary claims require extraordinary evidence.\cite{sagandemon}
\i It is better to explicitly display your assumptions rather than implicitly hold them.
\i It is a {\em good thing} to update your beliefs when you receive new information.
\i Not all uncertainties are the same.
\ei


There is not a universal agreement for the translation of numerical probability values to qualitative terms in English (i.e. highly unlikely, somewhat unlikely, etc...).  One rough guide is shown in Table~\ref{table1}.  I will be following this convention throughout the book, but realize that the specific probability distinctions are a bit arbitrary.


\begin{table}
\begin{tabular}{cc}
term & probability \\\hline\hline
virtually impossible & 1/1,000,000\\
extremely unlikely & 0.01 (i.e. 1/100) \\
very unlikely & 0.05 (i.e. 1/20) \\
unlikely & 0.2 (i.e. 1/5) \\
slightly unlikely & 0.4 (i.e. 2/5) \\
even odds & 0.5 (i.e. 50-50) \\
slightly likely & 0.6 (i.e. 3/5) \\
likely & 0.8 (i.e. 4/5) \\
very likely & 0.95 (i.e. 19/20) \\
extremely likely & 0.99 (i.e. 99/100) \\
virtually certain & 999,999/1,000,000
\end{tabular}
\label{table1}
\caption{Rough guide for the conversion of qualitative labels to probability values. }
\end{table}

