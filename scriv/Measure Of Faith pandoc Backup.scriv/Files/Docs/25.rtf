{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 Cochin;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural

\f0\fs28 \cf0 ## <$Title>\
\
Dan Barker has written an Easter Challenge[@Barker:1992aa] for\
any Christian to come up with a seamless account of what happened on the\
day of the Resurrection.\
\
\\pquote\{The conditions of the challenge are simple and reasonable. In each of\
the four Gospels, begin at Easter morning and read to the end of the\
book: Matthew 28, Mark 16, Luke 24, and John 20-21. Also read Acts\
1:3-12 and Paul's tiny version of the story in I Corinthians 15:3-8.\
These 165 verses can be read in a few moments. Then, without omitting a\
single detail from these separate accounts, write a simple,\
chronological narrative of the events between the resurrection and the\
ascension: what happened first, second, and so on; who said what, when;\
and where these things happened.\
\
Since the gospels do not always give precise times of day, it is\
permissible to make educated guesses. The narrative does not have to\
pretend to present a perfect picture--it only needs to give at least one\
plausible account of all of the facts. Additional explanation of the\
narrative may be set apart in parentheses. \\emph\{The important condition\
to the challenge, however, is that not one single biblical detail be\
omitted.\}\}\
\
Andy Bannister has written a response to this challenge[@Bannister:aa], which we must admit is in fact consistent with every detail of the story and supposed contradiction.  In many ways it is impressive.  \
\
\\todo\{describe his method here quickly\}\
\
\
\
However, there is a significant problem with the method he employs, which can be elucidated with an analogy to a situation which commonly arises in mathematics, in the process of fitting a line to data.\
\
\\subsection\{Regression\}\\label\{regression\}\
\
In linear regression, we might have, say, a handful of data points:\
\
\\vspace\{.1in\}\
\\begin\{tabular\}\{cc\}\
\\toprule\
\\textbf\{X\} & \\textbf\{Y\}\\\\\
0.0 & -12.5\\\\\
1.0 & 17.9\\\\\
2.0 & -3.2\\\\\
3.0 & 11.4\\\\\
4.0 & 10.8\\\\\
5.0 & 28.8\\\\\
6.0 & 31.8\\\\\
7.0 & 28.3\\\\\
8.0 & 23.4\\\\\
9.0 & 21.9\\\\\
10.0 & 32.7\\\\\
\\bottomrule\
\\end\{tabular\}\
\\vspace\{.1in\}\
\
Which is plotted as\
\
\\includegraphics\{fig1.png\}\
\
When we do a linear regression, we fit to a standard ``$y=mx+b$'' form.\
For this data, the best fit is\
\\beqn\
y=3.4 x + 0.27,\
\\eeqn\
with a mean squared error of 77.9.  This difference from the line to the data is one measure of how well the line compares to the data - lower values means a better fit, a mean squared error of zero means a \{\\em perfect\} fit.\
\
\\includegraphics\{fig2.png\}\
\
Overall, not a bad looking fit. However, if we fit to a more complex function, say a 10$^\{\\rm th\}$ polynomial, we can get even better!\
\
\\beqn\
y&=&-0.0007039 x^\{10\}  + 0.0363 x^9 - 0.8051 x^8 + 10.04 x^7 - 77.13 x^6 +\\\\\
&& 376.5 x^5- 1159 x^4 + 2152 x^3 - 2163 x^2 + 891.8 x^1 - 12.54\
\\eeqn\
\
with a Mean Squared Error of \{\\em zero\}.\
\
\\includegraphics\{fig4.png\}\
\
In the field of statistics, this is referred to as \{\\em over-fitting\}, and is\
the result of fitting the variation and not the overall pattern. In other words, it is fitting the\
meaningless differences from one point to another by adding a tunable\
parameter for each detail in the data. With each new parameter we get a\
``better'' fit, by the criterion of mean squared error, but we lose\
sight of the meaning. This is the mathematical equivalent of losing\
sight of the forest for the trees.\
\
\\subsection\{Parameters and Ockham's Razor\}\
\
When fitting a line, the result is choosing the values of the parameters that have the highest probability given the data.  However, when those parameters can take on any possible value, the overall probability of the model is reduced - this is the probabilistic equivalent of Ockham's Razor which we've seen before in Section~\\ref\{sec:ockham\} on page~\\pageref\{sec:ockham\}.  \
\
We make the problem worse by adding more and more parameters, giving more possible explanatory freedom to our model.  The more freedom we have in choosing the parts of the model, the less explanatory power this model actually has.  One way that statisticians avoid this problem is to fit the model to half of the data, and see how it works on the other half - a process called cross-validation.  A simple model, like a straight line, will do about as well on both.  An overly complex one will do well on the data used to fit it, but will do poorly on new data.\
\
\\todo\{do an example of this in this case\}\
\
\
\\subsection\{Back to the Resurrection\}\\label\{back-to-the-resurrection\}\
\
If we look at Andy Bannister's very clever solution, we notice something\
quite interesting: for every single difference between the Gospel\
accounts, he adds a detail not found in the story to explain it. One can\
pretty much do this for any two accounts that don't say logically\
contradictory things, and can be seen as an example of over-fitting. \
\
Another way of looking at it is that, if Bannister had constructed his story from, say, two of the Gospel stories and then compared it to the other two he'd have a problem.  Even if he took details from all four accounts, but constructed a story from half of them and then used the other half to confirm it wouldn't work.\
\
As a result, the Easter Challenge, as phrased, is probably not a very good one. One can Rube-Goldberg a story together to fit any amount of details.  Perhaps that is the point, to highlight to what lengths someone has to go to in order to reconcile the four accounts.  However, we think something motivated from cross-validation might be a bit more persuasive.\
}