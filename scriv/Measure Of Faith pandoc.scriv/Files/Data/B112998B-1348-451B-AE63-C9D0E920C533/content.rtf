{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 Cochin;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural

\f0\fs28 \cf0 #### Rule 4 (Bayes' rule):\
\
 This rule is perhaps the most obtuse to see for the first time, but is by far the most important rule of them all, so it is worth the effort.  Because of this, we will choose to write it in a somewhat more elaborated form, and rewrite it several ways.\
\\beqn\
\\Pg\{explanation\}\{data\} = \\frac\{\\Pg\{data\}\{explanation\}\\P\{explanation\}\}\{\\P\{data\}\}\
\\eeqn\
where each term is described more fully as \
\
1. $\\P\{explanation\}$ - this is the probability the explanation is correct *prior* to seeing the data.  The term itself is often called the *prior*, and represents your beliefs before you see the data.  Typically, more complex explanations are less likely a-
\i priori
\i0  than simpler ones.  \
2. $\\Pg\{explanation\}\{data\}$ - this is the probability the explanation is correct *after* seeing the data (a-
\i posteriori
\i0 ). The term itself is often called the *posterior* for this reason, and represents your updated beliefs once you have data.  Thus, Bayes' rule is a mathematical expression of *learning* from evidence.  \
3. $\\Pg\{data\}\{explanation\}$ - this is the probability that the data can be explained with this particular explanation.  The term itself is often called the *likelihood*, and can be thought of as a measure of how well the explanation fits the data.  If the explanation fits the data well, this number will be high, for example. \
4. $\\P\{data\}$ - this is the probability of the data, regardless of the explanation.  It is easiest to understand this term with an example.\
\
}